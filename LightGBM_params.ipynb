{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "LightGBM_params.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThU82_ZQTv9_"
      },
      "source": [
        "### LightGBM \n",
        "> Gradient Boosting Decision Tree with Gradient-based One-Side Sampling and Exclusive Feature Bundling   \n",
        "> GBDT with GOSS and EFB   \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb5kzrVSTv-L"
      },
      "source": [
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJwShC7gTv-M"
      },
      "source": [
        "[python API](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)   \n",
        "   \n",
        "+ lightgbm.LGGMModel \n",
        "  + \n",
        "  ``` \n",
        "  lightgbm.LGBMModel(boosting_type='gbdt', \n",
        "                       num_leaves=31, \n",
        "                       max_depth=- 1, \n",
        "                       learning_rate=0.1, \n",
        "                       n_estimators=100, \n",
        "                       subsample_for_bin=200000, \n",
        "                       objective=None, \n",
        "                       class_weight=None, \n",
        "                       min_split_gain=0.0, \n",
        "                       min_child_weight=0.001, \n",
        "                       min_child_samples=20, \n",
        "                       subsample=1.0, \n",
        "                       subsample_freq=0, \n",
        "                       colsample_bytree=1.0, \n",
        "                       reg_alpha=0.0, \n",
        "                       reg_lambda=0.0, \n",
        "                       random_state=None, \n",
        "                       n_jobs=- 1, \n",
        "                       silent=True, \n",
        "                       importance_type='split', \n",
        "                       **kwargs)\n",
        "  ```\n",
        "+ lightgbm.LGBMClassifier \n",
        "  + \n",
        "  ```\n",
        "  lightgbm.LGBMClassifier(boosting_type='gbdt', \n",
        "                            num_leaves=31, \n",
        "                            max_depth=- 1, \n",
        "                            learning_rate=0.1, \n",
        "                            n_estimators=100, \n",
        "                            subsample_for_bin=200000, \n",
        "                            objective=None, \n",
        "                            class_weight=None, \n",
        "                            min_split_gain=0.0, \n",
        "                            min_child_weight=0.001, \n",
        "                            min_child_samples=20, \n",
        "                            subsample=1.0, \n",
        "                            subsample_freq=0, \n",
        "                            colsample_bytree=1.0, \n",
        "                            reg_alpha=0.0, \n",
        "                            reg_lambda=0.0, \n",
        "                            random_state=None, \n",
        "                            n_jobs=- 1, \n",
        "                            silent=True, \n",
        "                            importance_type='split', \n",
        "                            **kwargs)\n",
        "  ```\n",
        "+ lightgbm.LGBMRegressor \n",
        "  + \n",
        "  ```\n",
        "  lightgbm.LGBMRegressor(boosting_type='gbdt', \n",
        "                           num_leaves=31, \n",
        "                           max_depth=- 1, \n",
        "                           learning_rate=0.1, \n",
        "                           n_estimators=100, \n",
        "                           subsample_for_bin=200000, \n",
        "                           objective=None, \n",
        "                           class_weight=None, \n",
        "                           min_split_gain=0.0, \n",
        "                           min_child_weight=0.001, \n",
        "                           min_child_samples=20, \n",
        "                           subsample=1.0, \n",
        "                           subsample_freq=0, \n",
        "                           colsample_bytree=1.0, \n",
        "                           reg_alpha=0.0, \n",
        "                           reg_lambda=0.0, \n",
        "                           random_state=None, \n",
        "                           n_jobs=- 1, \n",
        "                           silent=True, \n",
        "                           importance_type='split', \n",
        "                           **kwargs)\n",
        "  ```\n",
        "+ lightgbm.LGBMRanker "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oQxaIcjpiT7"
      },
      "source": [
        "+ boosting_type (***)\n",
        "  + gbdt(default : Gradient Bossted Decision Trees , dart : Drop out Regression Tress , goss : Gradient based One-Side Sampling) \n",
        "  + 이론적인 이해를 해야 선택이 가능할 것 같다.\n",
        "  + 보통 기본 값으로 쓴다고 하며 정확도가 중요할 때는 딥러닝 드랍아웃 같은 dart를 선택한다고 하는데 항상 좋은 예측력이 도출되는 것은 아니라고 한다. goss는 계산속도가 줄지만 예측력도 감소할 수 있다고 한다. \n",
        "\n",
        "+ num_leaves (*) ; num_leaves=31\n",
        "  + decision tree는 분기 별 잎사귀가 두개로 나뉘는 이진트리 형태이기 때문에 n이 깊이라고 할 때 $2^n$ 형태로 최대 잎사귀 수가 정해진다. \n",
        "  + 때문에 최대 잎사귀 수보다 큰 잎사귀를 주는 것은 별 의미가 없을 것 같다.\n",
        "  + 최대 잎사귀 수 보다 적게 지정하면 일종의 규제로 작용한다고 한다.\n",
        "\n",
        "+ max_depth (***) ; max_depth=-1\n",
        "  + 기본 설정은 제한 없는 분기이다.\n",
        "  + feature가 많은 경우 높게 설정하는 것이 좋을 것이다.\n",
        "  + 부스팅 방법에서는 깊이가 짧은 것은 보정되기 때문에 그리 중요 하지 않다는 의견도 있다.\n",
        "\n",
        "+ learning_rate (***) ; learning_rate=0.1\n",
        "  + 미세한 정확도 조정을 원할 때 학습률을 줄이고 다른 파라미터를 튜닝한다고 한다.\n",
        "  + 보통 0.05~0.1로 시작하고 긴 소수점으로 조정하는 것은 큰 의미가 없다고 한다.\n",
        "  + boosting 반복을 얼마나 많이 할지를 지정하는 변수와 잘 조합해서 사용해야고 한다. num_iterations이나 num_boost_round 변수로 반복 수를 조정하는데 기본 값이 100이어서 너무 적다는 경험적인 의견이 많았다. 보통 학습할 때 1000 이상을 주고 early stopping을 조건으로 걸어둔다고 한다.\n",
        "\n",
        "+ n_estimators () ; default = 100\n",
        "  + 얼마나 많은 부스팅 나무를 적합해 볼 것인지를 지정하는 인자.\n",
        "  + 별다른 언급이 없다.\n",
        "\n",
        "+ subsample_for_bin () ; default =200000\n",
        "  + 나무 만들 때 사용할 샘플 수\n",
        "  + 별다른 언급이 없다.\n",
        "\n",
        "+ objective=None, \n",
        "  + 풀려는 문제에 따라 binary, multiclass regression으로 알맞게 지정.\n",
        "  + 별 다른 언급은 없다.\n",
        "\n",
        "+ class_weight=None\n",
        "  + balanced 나 None지정 가능\n",
        "  + 지정시 {class_label : weight} 형태로 연결된 값을 불러와서 사용하게 된다.\n",
        "  + sample_weight를 지정할 경우 그 가중치와 곱해서 계산될 것.\n",
        "  + 예측력에 영향 있으므로 신중할 것\n",
        "  + 이진 분류라면 is_nubalance or scale_pos_weight 인자로 조정\n",
        "\n",
        "+ min_split_gain=0.0\n",
        "  + 분기가 되게 하는 최소 정보 이득을 지정. \n",
        "  + 유용한 분기의 수를 조정하는데 사용한다고 한다.\n",
        "\n",
        "+ min_child_weight=0.001\n",
        "  + 하나의 나뭇잎에서 가져야 하는 최소 헤시안 행렬 합\n",
        "  + 별 다른 언급은 없다.\n",
        "\n",
        "+ min_child_samples=20\n",
        "  + 하나의 나뭇잎에서 가져야 하는 최소 데이터의 수\n",
        "  + 별다른 언급은 없다.\n",
        "  + 너무 줄이면 과적합 위험이 있을 듯.\n",
        "\n",
        "+ subsample (**) ; default=1.0\n",
        "  + 행 샘플링\n",
        "  + 데이터를 일부 발췌해서 다양성을 높이는 방법으로 사용한다고 한다. 민감한 옵션이므로 column sampling과 잘 섞어서 쓴다고 한다.\n",
        "  + goss 는 그래디언트가 큰 데이터 인스턴스만을 샘플링 해서 쓰기 때문에 goss를 사용하고 이 옵션을 조정하면 에러가 난다고 한다.\n",
        "\n",
        "+ subsample_freq=0\n",
        "   + subsampling을 얼마나 자주할 것인지를 지정\n",
        "   + 기본 값은 하지 않도록 지정함\n",
        "\n",
        "+ colsample_bytree (**) ; default=1.0\n",
        "  + 열 샘플링. 컬럼에 대한 샘플링을 수행. random forest에서 수행하는 기능.\n",
        "  + 정확도가 높아지는 측면이 있다고 한다.\n",
        "  + 열 샘플링을 하지 않는 1이 기본이나, 0.7~0.9 설정이 일반적이라고 한다.\n",
        "\n",
        "+  reg_alpha (*) ; default=0.0\n",
        "  + L1정규화로 오버피팅을 막도록 하는 인자.\n",
        "  + 어떤 영향을 줄지 예측이 어려워 보통 조정하지 않는다고 한다.\n",
        "\n",
        "+  reg_lambda (*) ; default=0.0\n",
        "  + L2 정규화. 보통 조정하지 않는다고 한다.\n",
        "\n",
        "+ random_state=None\n",
        "  + 시드 주기\n",
        "\n",
        "+ n_jobs=- 1\n",
        "  + 병렬 처리 스레드 수\n",
        "\n",
        "+ silent=True\n",
        "  + 부스팅 수행 시 메시지 표시 여부 \n",
        "\n",
        "+  importance_type='split'\n",
        "  + 'split' 또는 'gain'\n",
        "  + feature_importance_ 에 저장되는 값을 어떻게 정할지를 지정.\n",
        "  + split이면 해당 피쳐 컬럼이 모델에 사용된 횟수를 값으로 한다.\n",
        "  + gain이면 해당 피쳐를 사용한 분기에서의 총 gain을 값으로 한다.\n",
        "\n",
        "---\n",
        "\n",
        "+ metric (***)\n",
        "  + 학습 목적에 따라 metric을 잘 설정해야 한다.\n",
        "  + binary cross entropy\n",
        "  + multiclass cross entropy\n",
        "  + regression L2 : mse\n",
        "  + regression L1 : mae\n",
        "  + mape\n",
        "  + poisson (log trasnformation)\n",
        "  + quantile \n",
        "  + huber ( huber loss, mae approx )\n",
        "  + fair ( fair loss, mae approx )\n",
        "  + gamma ( residual deviance )\n",
        "  + lambdarank\n",
        "  + tweedie\n",
        "\n",
        "+ early_stopping_round / early_stopping_rounds (**)\n",
        "  + validation으로 평가하여 발전이 없는 것이 얼마나 반복되었을 때 모델 적합을 중단할지를 설정.\n",
        "  + validation 데이터가 있어야 한다.\n",
        "\n",
        "+ enable_bundle / enable_feature_grouping (*)\n",
        "  + 피처 번들링에 대한 여부. 보통은 라벨인코딩을 직접해주고 피처 번들링 기능을 끄는것이 희한한 결과를 보지 않는 방법 중 하나다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEi9pMI5tfG9"
      },
      "source": [
        "#### 더 빠른 속도를 위하여 :\n",
        "\n",
        "+ bagging_fraction/subsample 과 baggin_freq/subsample_freq 을 설정하여 bagging 을 적용하십시오   \n",
        "+ feature_fraction/ colsample_bytree 을 설정하여 feature sub-sampling을 하십시오   \n",
        "+ 작은 max_bin 값을 사용하십시오   \n",
        "+ save_binary 를 값을 통해 다가오는 학습에서 데이터 로딩 속도를 줄이십시오   \n",
        "+ parallel learning/n_jobs 병렬 학습을 적용하십시오   \n",
        "\n",
        "#### 더 나은 정확도를 위해 :\n",
        "\n",
        "+ 큰 max_bin 값을 사용하십시오 (아마 속도는 느려질 수 있습니다)   \n",
        "+ 작은 learning_rate 값을 큰 num_iterations 값과 함께 사용하십시오   \n",
        "+ 큰 num_leaves 값을 사용하십시오 (아마 과적합을 유발할 수도 있습니다)   \n",
        "+ 더 큰 트레이닝 데이터를 사용하십시오   \n",
        "+ dart 를 사용하십시오   \n",
        "+ 범주형 feature를 사용하십시오   \n",
        "\n",
        "#### 과적합을 해결하기 위해 :\n",
        "\n",
        "+ 작은 max_bin 값을 사용하십시오   \n",
        "+ 작은 num_leaves 값을 사용하십시오   \n",
        "+ min_data_in_leaf/min_child_samples 와 min_sum_hessian_in_leaf/min_child_weight 파라미터를 사용하십시오   \n",
        "+ bagging_fraction/sumbsample 과 bagging_freq/subsample_freq 을 사용하여 bagging 을 적용하십시오   \n",
        "+ feature_fraction/colsample_byree 을 세팅하여 feature sub-sampling을 하십시오   \n",
        "+ lambda_l1/reg_alpha, lambda_l2/reg_lambda 그리고 min_gain_to_split 파라미터를 이용해 regularization (정규화) 를 적용하십시오   \n",
        "+ max_depth 를 설정해 Deep Tree 가 만들어지는 것을 방지하십시오   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8I2_LCQ3I1v"
      },
      "source": [
        "\n",
        "\n",
        "+ [paper](https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)\n",
        "+ [medium](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)   \n",
        "+ [parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html#)\n",
        "+ [parameters](http://machinelearningkorea.com/2019/09/29/lightgbm-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0/)\n",
        "\n",
        "+ [주요](http://machinelearningkorea.com/2019/09/29/lightgbm-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0/)\n",
        "+ [모델성능비교](https://blog.linewalks.com/archives/6522)\n",
        "+ [feature importance](https://www.kaggle.com/ashishpatel26/feature-importance-of-lightgbm)\n",
        "+ [dart](https://dacon.io/competitions/official/235573/codeshare/693)\n",
        "+ [permutation](https://soohee410.github.io/iml_permutation_importance)\n",
        "+ [permutation](https://hong-yp-ml-records.tistory.com/51)\n",
        "+ [permutaton](https://scikit-learn.org/stable/modules/permutation_importance.html)\n",
        "+ [파라미터 튜닝](https://nurilee.com/2020/04/03/lightgbm-definition-parameter-tuning/)\n",
        "+ [주요 파라미터](https://gorakgarak.tistory.com/1285)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvQXY2Y43JWO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}